{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3b7c4-934f-41c6-b21e-c5ad903be1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import requests\n",
    "import mpu\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549d0c27-e21c-4892-8de8-59ef9a72bc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DEPARTURE_ZIPCODE</th>\n",
       "      <th>ARRIVAL_ZIPCODE</th>\n",
       "      <th>GROSS_WEIGHT_KG</th>\n",
       "      <th>NET_WEIGHT_KG</th>\n",
       "      <th>VOLUME_M3</th>\n",
       "      <th>DECLARED_DISTANCE_KM</th>\n",
       "      <th>DELIVERY_TIME_HH</th>\n",
       "      <th>WDAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14554.000000</td>\n",
       "      <td>14548.000000</td>\n",
       "      <td>14554.000000</td>\n",
       "      <td>14554.000000</td>\n",
       "      <td>14554.000000</td>\n",
       "      <td>14554.000000</td>\n",
       "      <td>14554.000000</td>\n",
       "      <td>14554.000000</td>\n",
       "      <td>14554.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7277.500000</td>\n",
       "      <td>53013.321694</td>\n",
       "      <td>41699.581352</td>\n",
       "      <td>9.744264</td>\n",
       "      <td>8.002172</td>\n",
       "      <td>0.188305</td>\n",
       "      <td>925.313013</td>\n",
       "      <td>81.338395</td>\n",
       "      <td>1.730177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4201.522244</td>\n",
       "      <td>22512.124864</td>\n",
       "      <td>30222.758997</td>\n",
       "      <td>34.946037</td>\n",
       "      <td>31.718243</td>\n",
       "      <td>0.983377</td>\n",
       "      <td>1320.171381</td>\n",
       "      <td>73.386400</td>\n",
       "      <td>1.507748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3639.250000</td>\n",
       "      <td>56122.750000</td>\n",
       "      <td>17031.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>240.940000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7277.500000</td>\n",
       "      <td>63076.000000</td>\n",
       "      <td>35042.000000</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>428.230000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10915.750000</td>\n",
       "      <td>63076.000000</td>\n",
       "      <td>66020.000000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>2.280000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>547.950000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14554.000000</td>\n",
       "      <td>99208.000000</td>\n",
       "      <td>99518.000000</td>\n",
       "      <td>1039.820000</td>\n",
       "      <td>948.840000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>11818.310000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  DEPARTURE_ZIPCODE  ARRIVAL_ZIPCODE  GROSS_WEIGHT_KG  \\\n",
       "count  14554.000000       14548.000000     14554.000000     14554.000000   \n",
       "mean    7277.500000       53013.321694     41699.581352         9.744264   \n",
       "std     4201.522244       22512.124864     30222.758997        34.946037   \n",
       "min        1.000000          12.000000        10.000000         0.020000   \n",
       "25%     3639.250000       56122.750000     17031.000000         1.000000   \n",
       "50%     7277.500000       63076.000000     35042.000000         1.370000   \n",
       "75%    10915.750000       63076.000000     66020.000000         3.160000   \n",
       "max    14554.000000       99208.000000     99518.000000      1039.820000   \n",
       "\n",
       "       NET_WEIGHT_KG     VOLUME_M3  DECLARED_DISTANCE_KM  DELIVERY_TIME_HH  \\\n",
       "count   14554.000000  14554.000000          14554.000000      14554.000000   \n",
       "mean        8.002172      0.188305            925.313013         81.338395   \n",
       "std        31.718243      0.983377           1320.171381         73.386400   \n",
       "min         0.000000      0.000000              0.640000          0.000000   \n",
       "25%         0.000000      0.010000            240.940000         38.000000   \n",
       "50%         0.000000      0.010000            428.230000         72.000000   \n",
       "75%         2.280000      0.110000            547.950000        108.000000   \n",
       "max       948.840000    105.000000          11818.310000       1920.000000   \n",
       "\n",
       "               WDAY  \n",
       "count  14554.000000  \n",
       "mean       1.730177  \n",
       "std        1.507748  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        1.000000  \n",
       "75%        3.000000  \n",
       "max        6.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"./delivery_data_pulito.csv\")#lines=true perchè ogni riga del file.json è un json, devo creare tabella\n",
    "\n",
    "#dataset.info()\n",
    "\n",
    "dataset.describe()\n",
    "#dataset['VEHICLETYPE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ff878-1f19-43c5-87ac-7c5937a7c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi il file CSV originale\n",
    "df = pd.read_csv('delivery_data.csv')\n",
    "\n",
    "# Conta le righe con NaN prima di rimuoverle\n",
    "nan_count = df['SERVICETYPE'].isna().sum()\n",
    "\n",
    "# Rimuovi le righe con NaN in SERVICETYPE\n",
    "df = df.dropna(subset=['SERVICETYPE'])\n",
    "\n",
    "# Conta le spedizioni IT-US prima di rimuoverle\n",
    "it_us_count = (\n",
    "    ((df['DEPARTURE_COUNTRY'] == 'IT') & (df['ARRIVAL_COUNTRY'] == 'US')) | \n",
    "    ((df['DEPARTURE_COUNTRY'] == 'US') & (df['ARRIVAL_COUNTRY'] == 'IT'))\n",
    ").sum()\n",
    "\n",
    "# Rimuovi le spedizioni tra IT e US in entrambe le direzioni\n",
    "mask_it_us = ~(\n",
    "    ((df['DEPARTURE_COUNTRY'] == 'IT') & (df['ARRIVAL_COUNTRY'] == 'US')) | \n",
    "    ((df['DEPARTURE_COUNTRY'] == 'US') & (df['ARRIVAL_COUNTRY'] == 'IT'))\n",
    ")\n",
    "df = df[mask_it_us]\n",
    "\n",
    "# Crea i tre dataframe separati basati su SERVICETYPE\n",
    "# 1. Corrieri\n",
    "corrieri_df = df[df['SERVICETYPE'].isin(['Corriere espresso', 'Corriere espresso resi'])]\n",
    "\n",
    "# 2. Via gomma\n",
    "gomma_df = df[df['SERVICETYPE'] == 'Via gomma']\n",
    "\n",
    "# 3. E-commerce\n",
    "ecommerce_df = df[df['SERVICETYPE'].isin(['E-commerce', 'E-commerce Resi'])]\n",
    "\n",
    "# Salva i dataframe in file CSV separati\n",
    "corrieri_df.to_csv('corrieri_data.csv', index=False)\n",
    "gomma_df.to_csv('gomma_data.csv', index=False)\n",
    "ecommerce_df.to_csv('ecommerce_data.csv', index=False)\n",
    "\n",
    "# Stampa informazioni dettagliate\n",
    "print(\"Statistiche dei file generati:\")\n",
    "print(f\"\\nRighe nel file corrieri: {len(corrieri_df)}\")\n",
    "print(f\"Righe nel file gomma: {len(gomma_df)}\")\n",
    "print(f\"Righe nel file ecommerce: {len(ecommerce_df)}\")\n",
    "\n",
    "# Stampa le righe rimosse\n",
    "print(f\"\\nRighe rimosse con NaN in SERVICETYPE: {nan_count}\")\n",
    "print(f\"Righe rimosse per spedizioni IT-US: {it_us_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718f03e-640d-40dc-9a8a-3785b8a6dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Getting a zip distance matrix for easy lookup\n",
    "\n",
    "# Necessary Imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mpu\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Helper functions\n",
    "def walk_up_folder(path, depth=1):\n",
    "    \"\"\"\n",
    "    Helper method to navigate the file system and get to the file location\n",
    "    \"\"\"\n",
    "    _cur_depth = 1\n",
    "    while _cur_depth < depth:\n",
    "        path = os.path.dirname(path)\n",
    "        _cur_depth += 1\n",
    "    return path\n",
    "\n",
    "def geodist(coord1,coord2):\n",
    "    \"\"\"\n",
    "    Calculate the distance between\n",
    "    (lat1, lon1), (lat2, lon2)\n",
    "    \"\"\"\n",
    "    # Convert to miles 1km = 0.621371 miles\n",
    "    return round(mpu.haversine_distance(coord1,coord2)*0.621371,2)\n",
    "\n",
    "def fill_lfromu(A):\n",
    "    \"\"\"\n",
    "    Fill out the symmetric values in the lower triangle based on upper\n",
    "    Not needed since it is covered in the for loop above\n",
    "    \"\"\"\n",
    "    out = A.T + A\n",
    "    np.fill_diagonal(out,0)\n",
    "    return out\n",
    "\n",
    "import numba as nb\n",
    "#@nb.njit(parallel=True,fastmath=True)\n",
    "def func(len_matrix,coordinates):\n",
    "    zip_dist = np.zeros((len_matrix, ) * 2)\n",
    "    indices = np.arange(len_matrix)\n",
    "    # Get upper triangular pairs\n",
    "    fill_cells = np.stack(np.triu_indices(len_matrix), axis=1)\n",
    "    # Loop through upper triangular indices while avoiding diagonal element indices\n",
    "    for idx in fill_cells:\n",
    "        i,j = indices[idx]\n",
    "        if i!=j:\n",
    "            zip_dist[i][j]= zip_dist[j][i] = geodist(coordinates[i],coordinates[j])\n",
    "    return zip_dist\n",
    "\n",
    "# ## Loading raw data\n",
    "# \n",
    "# Source - https://github.com/symerio/postal-codes-data/blob/master/data/geonames/US.txt \n",
    "# \n",
    "# **Geocoding format**\n",
    "# The result of a geo-localistion query is a pandas.DataFrame with the following columns,\n",
    "# \n",
    "# country_code: iso country code, 2 characters  \n",
    "# postal_code : postal code  \n",
    "# place_name : place name (e.g. town, city etc)  \n",
    "# state_name : 1. order subdivision (state)  \n",
    "# state_code : 1. order subdivision (state)  \n",
    "# county_name : 2. order subdivision (county/province)  \n",
    "# county_code : 2. order subdivision (county/province)  \n",
    "# community_name : 3. order subdivision (community)  \n",
    "# community_code : 3. order subdivision (community)  \n",
    "# latitude : estimated latitude (wgs84)  \n",
    "# longitude : estimated longitude (wgs84)  \n",
    "# accuracy : accuracy of lat/lng from 1=estimated to 6=centroid  \n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    \n",
    "    data_dir = os.path.join(walk_up_folder(os.getcwd(), 2), \"Data/\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(\"Data folder created\")\n",
    "        \n",
    "    # Load USZips data\n",
    "    source_file = os.path.join(data_dir,\"US_zips.txt\")\n",
    "    if os.path.exists(source_file):\n",
    "        print(\"Loading the source file\")\n",
    "        print(\"Processing the dataframe\")\n",
    "        zip_df = pd.read_csv(source_file),sep='\\t',header=None,\n",
    "                             names=[\"country_code\",\"postal_code\",\"place_name\",\"state_name\",\"state_code\",\n",
    "                                    \"county_name\",\"county_code\",\"community_name\",\"community_code\",\n",
    "                                    \"latitude\",\"longitude\",\"accuracy\"])\n",
    "\n",
    "        # Dropping unnecessary columns\n",
    "        zip_df.drop([\"country_code\",\"place_name\",\"state_name\",\"county_name\",\"county_code\",\"community_name\",\"community_code\"],axis=1,inplace=True)\n",
    "        # There were 2 duplicate instances\n",
    "        zip_df.drop_duplicates(subset=[\"postal_code\"],keep='last',inplace=True)\n",
    "        # Coordinate column as (lat,long)\n",
    "        zip_df[\"coord\"]=list(zip(zip_df[\"latitude\"],zip_df[\"longitude\"]))\n",
    "        zip_df[\"mapping\"] = zip_df.index # Column with index\n",
    "\n",
    "        # ## Lookup Matrix\n",
    "        #\n",
    "        # The idea is to form a (41483,41483) matrix where the cell values are the haversize distances between two zips that are indexed.\n",
    "        # I'll save the index look up and the matrix file as json or pkl\n",
    "\n",
    "\n",
    "        # Dictionary mapping the zipcode to index\n",
    "        zip_index = dict(zip(zip_df[\"postal_code\"].astype(str),zip_df[\"mapping\"]))\n",
    "        print(len(zip_index))\n",
    "        # Saving the indexer\n",
    "        with open(os.path.join(data_dir, \"zips_indexer.json\")), 'w') as f:\n",
    "            json.dump(zip_index, f)\n",
    "        print(\"ZiptoIndex json saved!\")\n",
    "        \n",
    "        # Matrix\n",
    "        len_matrix = zip_df.shape[0]\n",
    "        coordinates = zip_df.coord.values\n",
    "        \n",
    "        print(\"Creating the pairwise distance matrix! This might take an hour, why don't you go get something to eat? \")\n",
    "        # Takes 1hour. Unfortunately numba.jit doesn't work with user defined functions \")\n",
    "        zip_dist = func(len_matrix,coordinates)\n",
    "        # Convert to float32\n",
    "        print(\"ZipDist Matrix saving!\")\n",
    "        np.savez_compressed(os.path.join(data_dir, \"zip_dist.npz\")), np.float32(zip_dist))\n",
    "    else:\n",
    "        print(\"Please ensure you have the source file of the raw zips data in the Data directory and try again !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee524dc-e104-4167-a552-662ab21b75c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 100/574 [03:27<15:53,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistiche dopo 100 righe processate ---\n",
      "Ricerche riuscite: 100 (100.00%)\n",
      "Ricerche fallite: 0 (0.00%)\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 200/574 [06:52<12:26,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistiche dopo 200 righe processate ---\n",
      "Ricerche riuscite: 200 (100.00%)\n",
      "Ricerche fallite: 0 (0.00%)\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 300/574 [10:13<09:09,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistiche dopo 300 righe processate ---\n",
      "Ricerche riuscite: 300 (100.00%)\n",
      "Ricerche fallite: 0 (0.00%)\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 400/574 [13:34<05:49,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistiche dopo 400 righe processate ---\n",
      "Ricerche riuscite: 400 (100.00%)\n",
      "Ricerche fallite: 0 (0.00%)\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 500/574 [16:58<02:29,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistiche dopo 500 righe processate ---\n",
      "Ricerche riuscite: 500 (100.00%)\n",
      "Ricerche fallite: 0 (0.00%)\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [19:32<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistiche dopo 574 righe processate ---\n",
      "Ricerche riuscite: 574 (100.00%)\n",
      "Ricerche fallite: 0 (0.00%)\n",
      "------------------------------------------------\n",
      "\n",
      "Elaborazione completata. Dati salvati in validated_delivery_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from geopy.geocoders import Nominatim\n",
    "from typing import Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Carica il dataset\n",
    "dataSet = pd.read_csv(\"validated_delivery_data.csv\")\n",
    "\n",
    "def get_osrm_distance(coord1, coord2):\n",
    "    url = f\"http://router.project-osrm.org/route/v1/driving/{coord1[1]},{coord1[0]};{coord2[1]},{coord2[0]}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if 'routes' in data and len(data['routes']) > 0:\n",
    "        distance_km = data['routes'][0]['distance'] / 1000\n",
    "        duration_seconds = data['routes'][0]['duration']\n",
    "        return round(distance_km, 2), duration_seconds\n",
    "    return None, None\n",
    "\n",
    "def get_zipcode_coordinates(zipcode: str, country) -> Optional[Tuple[float, float]]:\n",
    "    try:\n",
    "        geolocator = Nominatim(user_agent=\"my_distance_calculator\")\n",
    "        location = geolocator.geocode(f\"{zipcode}, {country}\")\n",
    "        \n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "        return None\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def print_statistics(processed_rows, successful_lookups, failed_lookups):\n",
    "    total = successful_lookups + failed_lookups\n",
    "    success_rate = (successful_lookups / total * 100) if total > 0 else 0\n",
    "    failure_rate = (failed_lookups / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- Statistiche dopo {processed_rows} righe processate ---\")\n",
    "    print(f\"Ricerche riuscite: {successful_lookups} ({success_rate:.2f}%)\")\n",
    "    print(f\"Ricerche fallite: {failed_lookups} ({failure_rate:.2f}%)\")\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "\n",
    "# Filtra il DataFrame per le righe non ancora calcolate\n",
    "filtered_data = dataSet[\n",
    "    (dataSet['DECLARED_DISTANCE_KM'].isna() | (dataSet['DECLARED_DISTANCE_KM'] == 0))\n",
    "]\n",
    "\n",
    "# Contatori per le statistiche\n",
    "processed_rows = 0\n",
    "successful_lookups = 0\n",
    "failed_lookups = 0\n",
    "\n",
    "# Itera attraverso il dataset filtrato\n",
    "for index, row in tqdm(filtered_data.iterrows(), total=filtered_data.shape[0]):\n",
    "    processed_rows += 1\n",
    "    \n",
    "    departure_country = row['DEPARTURE_COUNTRY']\n",
    "    departure_zipcode = row['DEPARTURE_ZIPCODE']\n",
    "    arrival_zipcode = row['ARRIVAL_ZIPCODE']\n",
    "    \n",
    "    if departure_country in [\"IT\", \"US\"]:\n",
    "        coordinataPartenza = get_zipcode_coordinates(departure_zipcode, departure_country)\n",
    "        coordinataArrivo = get_zipcode_coordinates(arrival_zipcode, departure_country)\n",
    "    else:\n",
    "        failed_lookups += 1\n",
    "        continue\n",
    "    \n",
    "    if coordinataPartenza and coordinataArrivo:\n",
    "        distanza = None\n",
    "        error_count = 0\n",
    "        \n",
    "        while error_count < 5:\n",
    "            distanza, _ = get_osrm_distance(coordinataPartenza, coordinataArrivo)\n",
    "            if distanza is not None:\n",
    "                # Aggiorna il DataFrame e salva immediatamente\n",
    "                dataSet.at[index, 'DECLARED_DISTANCE_KM'] = distanza\n",
    "                dataSet.to_csv(\"validated_delivery_data.csv\", index=False)\n",
    "                \n",
    "                successful_lookups += 1\n",
    "                break\n",
    "            error_count += 1\n",
    "            \n",
    "            if error_count == 5:\n",
    "                print(f\"Impossibile trovare la distanza id {index}\")\n",
    "                failed_lookups += 1\n",
    "    else:\n",
    "        failed_lookups += 1\n",
    "    \n",
    "    # Stampa le statistiche ogni 100 righe\n",
    "    if processed_rows % 100 == 0:\n",
    "        print_statistics(processed_rows, successful_lookups, failed_lookups)\n",
    "\n",
    "# Stampa le statistiche finali\n",
    "print_statistics(processed_rows, successful_lookups, failed_lookups)\n",
    "\n",
    "print(\"Elaborazione completata. Dati salvati in validated_delivery_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39bd6273-ce70-4841-8d41-c188a3798c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulizia dei dati completata.\n",
      "1. Dataset con chilometri integrati salvato come 'km_integrated_delivery_data.csv'\n",
      "2. Dataset con chilometri e tempi sistemati salvato come 'validated_delivery_data.csv'\n"
     ]
    }
   ],
   "source": [
    "#creazione dataset uniti con orari giusti\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_delivery_data(main_file, additional_files):\n",
    "    \"\"\"\n",
    "    Procedura di pulizia e integrazione dei dati di consegna\n",
    "    \n",
    "    Args:\n",
    "    main_file (str): Percorso del file CSV principale\n",
    "    additional_files (list): Lista di percorsi dei file CSV aggiuntivi\n",
    "    \n",
    "    Returns:\n",
    "    tuple: DataFrame con chilometri integrati e DataFrame con chilometri e tempi sistemati\n",
    "    \"\"\"\n",
    "    # 1. Carica il dataset principale\n",
    "    df = pd.read_csv(main_file)\n",
    "    \n",
    "    # 2. Crea una copia del dataset\n",
    "    df_km_integrated = df.copy()\n",
    "    \n",
    "    # 3. Cerca di riempire i chilometri mancanti dagli altri dataset\n",
    "    for additional_file in additional_files:\n",
    "        additional_df = pd.read_csv(additional_file)\n",
    "        \n",
    "        # Trova le righe con chilometri mancanti\n",
    "        mask_missing_km = df_km_integrated['DECLARED_DISTANCE_KM'].isna()\n",
    "        \n",
    "        # Cerca di integrare i chilometri dall'altro dataset\n",
    "        for index, row in df_km_integrated[mask_missing_km].iterrows():\n",
    "            match = additional_df[\n",
    "                (additional_df['DEPARTURE_COUNTRY'] == row['DEPARTURE_COUNTRY']) & \n",
    "                (additional_df['ARRIVAL_COUNTRY'] == row['ARRIVAL_COUNTRY'])\n",
    "            ]\n",
    "            \n",
    "            if not match.empty:\n",
    "                df_km_integrated.at[index, 'DECLARED_DISTANCE_KM'] = match['DECLARED_DISTANCE_KM'].mode().iloc[0]\n",
    "    \n",
    "    # 4. Crea un ulteriore copia per gestire i tempi di viaggio negativi\n",
    "    df_validated = df_km_integrated.copy()\n",
    "    \n",
    "    # 5. Gestisci tempi di viaggio negativi\n",
    "    negative_travel_time = df_validated['DELIVERY_TIME_HH'] < 0\n",
    "    \n",
    "    # Converti tempi negativi in positivi e scambia le date\n",
    "    df_validated.loc[negative_travel_time, 'DELIVERY_TIME_HH'] = abs(df_validated.loc[negative_travel_time, 'DELIVERY_TIME_HH'])\n",
    "    df_validated.loc[negative_travel_time, ['SHIPPING_DATE', 'ACTUAL_DELIVERY_DATE']] = \\\n",
    "        df_validated.loc[negative_travel_time, ['ACTUAL_DELIVERY_DATE', 'SHIPPING_DATE']].values\n",
    "    \n",
    "    return df_km_integrated, df_validated\n",
    "\n",
    "# Esempio di utilizzo\n",
    "main_file = 'delivery_data.csv'\n",
    "additional_files = ['gomma_data.csv', 'ecommerce_data.csv', 'corrieri_data.csv']\n",
    "\n",
    "# Esegui la pulizia dei dati\n",
    "km_integrated_data, validated_data = clean_delivery_data(main_file, additional_files)\n",
    "\n",
    "# Salva i due dataset\n",
    "km_integrated_data.to_csv('km_integrated_delivery_data.csv', index=False)\n",
    "validated_data.to_csv('validated_delivery_data.csv', index=False)\n",
    "\n",
    "print(\"Pulizia dei dati completata.\")\n",
    "print(\"1. Dataset con chilometri integrati salvato come 'km_integrated_delivery_data.csv'\")\n",
    "print(\"2. Dataset con chilometri e tempi sistemati salvato come 'validated_delivery_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "698d8329-51d0-4747-ba45-c669b03bf92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuratezza SERVICETYPE: 96.72%\n",
      "Accuratezza VEHICLETYPE: 90.55%\n",
      "\n",
      "File 'validated_delivery_data_ml_imputed.csv' creato con successo.\n"
     ]
    }
   ],
   "source": [
    "#sistemazione tipi di veicoli e servizzi definitivo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "\n",
    "# Sopprime gli specifici warning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If no columns specified, use all columns\n",
    "        if self.columns is None:\n",
    "            self.columns = X.columns\n",
    "\n",
    "        # Fit label encoders for specified columns\n",
    "        for col in self.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X[col].astype(str).fillna('Unknown'))\n",
    "            self.encoders[col] = le\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for col in self.columns:\n",
    "            # Check if column exists and has an encoder\n",
    "            if col in X_copy.columns and col in self.encoders:\n",
    "                le = self.encoders[col]\n",
    "                \n",
    "                # Transform column, handling unseen and NaN values\n",
    "                X_copy[col] = X_copy[col].fillna('Unknown')\n",
    "                X_copy[col] = X_copy[col].map(\n",
    "                    lambda s: le.transform([str(s)])[0] \n",
    "                    if str(s) in le.classes_ \n",
    "                    else -1\n",
    "                )\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "def prepare_data_for_ml(df):\n",
    "    \"\"\"\n",
    "    Prepara i dati per l'addestramento del modello di Machine Learning\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Dataset originale\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Features, target per SERVICETYPE e VEHICLETYPE\n",
    "    \"\"\"\n",
    "    # Seleziona le feature per la predizione\n",
    "    features_columns = [\n",
    "        'DEPARTURE_COUNTRY', \n",
    "        'ARRIVAL_COUNTRY', \n",
    "        'GROSS_WEIGHT_KG', \n",
    "        'NET_WEIGHT_KG', \n",
    "        'VOLUME_M3', \n",
    "        'DECLARED_DISTANCE_KM',\n",
    "        'DELIVERY_TIME_HH',\n",
    "        'WDAY'\n",
    "    ]\n",
    "    \n",
    "    # Crea una copia del dataset\n",
    "    df_prepared = df.copy()\n",
    "    \n",
    "    # Rimuovi le righe con valori target nulli\n",
    "    df_known = df_prepared.dropna(subset=['SERVICETYPE', 'VEHICLETYPE'])\n",
    "    \n",
    "    # Prepara le features\n",
    "    X = df_known[features_columns].copy()\n",
    "    \n",
    "    # Prepara i target\n",
    "    y_service = df_known['SERVICETYPE']\n",
    "    y_vehicle = df_known['VEHICLETYPE']\n",
    "    \n",
    "    return X, y_service, y_vehicle\n",
    "\n",
    "def create_ml_pipeline(is_categorical=True):\n",
    "    \"\"\"\n",
    "    Crea una pipeline di preprocessing e classificazione\n",
    "    \n",
    "    Args:\n",
    "    is_categorical (bool): Se True, usa encoding per le colonne categoriche\n",
    "    \n",
    "    Returns:\n",
    "    Pipeline: Pipeline di preprocessing e classificazione\n",
    "    \"\"\"\n",
    "    # Colonne numeriche e categoriche\n",
    "    numeric_features = [\n",
    "        'GROSS_WEIGHT_KG', \n",
    "        'NET_WEIGHT_KG', \n",
    "        'VOLUME_M3', \n",
    "        'DECLARED_DISTANCE_KM',\n",
    "        'DELIVERY_TIME_HH',\n",
    "        'WDAY'\n",
    "    ]\n",
    "    categorical_features = ['DEPARTURE_COUNTRY', 'ARRIVAL_COUNTRY']\n",
    "    \n",
    "    # Preprocessing per colonne numeriche\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Preprocessing per colonne categoriche\n",
    "    preprocessing_steps = [\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "    ]\n",
    "    \n",
    "    # Aggiungi encoding se richiesto\n",
    "    if is_categorical:\n",
    "        preprocessing_steps.append(('encoder', MultiColumnLabelEncoder()))\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=preprocessing_steps)\n",
    "    \n",
    "    # Combina i preprocessori\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Pipeline completa con preprocessore e classificatore\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def impute_with_ml(input_file):\n",
    "    \"\"\"\n",
    "    Imputa i valori mancanti utilizzando Machine Learning\n",
    "    \n",
    "    Args:\n",
    "    input_file (str): Percorso del file CSV di input\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame con valori imputati\n",
    "    \"\"\"\n",
    "    # Carica il dataset\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Crea un duplicato del dataset\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Prepara i dati per l'addestramento\n",
    "    X, y_service, y_vehicle = prepare_data_for_ml(df)\n",
    "    \n",
    "    # Dividi i dati in training e test\n",
    "    X_train, X_test, y_service_train, y_service_test, y_vehicle_train, y_vehicle_test = train_test_split(\n",
    "        X, y_service, y_vehicle, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Crea e addestra i modelli\n",
    "    service_pipeline = create_ml_pipeline(is_categorical=True)\n",
    "    vehicle_pipeline = create_ml_pipeline(is_categorical=True)\n",
    "    \n",
    "    # Addestra entrambi i modelli\n",
    "    service_pipeline.fit(X_train, y_service_train)\n",
    "    vehicle_pipeline.fit(X_train, y_vehicle_train)\n",
    "    \n",
    "    # Valuta i modelli\n",
    "    service_pred = service_pipeline.predict(X_test)\n",
    "    vehicle_pred = vehicle_pipeline.predict(X_test)\n",
    "    \n",
    "    # Calcola l'accuratezza\n",
    "    service_accuracy = accuracy_score(y_service_test, service_pred) * 100\n",
    "    vehicle_accuracy = accuracy_score(y_vehicle_test, vehicle_pred) * 100\n",
    "    \n",
    "    print(f\"\\nAccuratezza SERVICETYPE: {service_accuracy:.2f}%\")\n",
    "    print(f\"Accuratezza VEHICLETYPE: {vehicle_accuracy:.2f}%\")\n",
    "    \n",
    "    # Identifica le righe con valori mancanti\n",
    "    missing_mask = df_imputed['SERVICETYPE'].isna() | df_imputed['VEHICLETYPE'].isna()\n",
    "    missing_data = df_imputed.loc[missing_mask, X.columns].copy()\n",
    "    \n",
    "    # Imputa i valori mancanti\n",
    "    for index in missing_mask[missing_mask].index:\n",
    "        # Imputa SERVICETYPE\n",
    "        if pd.isna(df_imputed.at[index, 'SERVICETYPE']):\n",
    "            df_imputed.at[index, 'SERVICETYPE'] = service_pipeline.predict(missing_data.loc[[index]])[0]\n",
    "        \n",
    "        # Imputa VEHICLETYPE\n",
    "        if pd.isna(df_imputed.at[index, 'VEHICLETYPE']):\n",
    "            df_imputed.at[index, 'VEHICLETYPE'] = vehicle_pipeline.predict(missing_data.loc[[index]])[0]\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Esegui l'imputazione\n",
    "input_file = 'validated_delivery_data.csv'\n",
    "df_with_imputed_data = impute_with_ml(input_file)\n",
    "\n",
    "# Salva il dataset con i valori imputati\n",
    "df_with_imputed_data.to_csv('validated_delivery_data_ml_imputed.csv', index=False)\n",
    "\n",
    "print(\"\\nFile 'validated_delivery_data_ml_imputed.csv' creato con successo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a009274d-8c90-463b-8701-6dbacad9b7be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 192\u001b[0m\n\u001b[0;32m    189\u001b[0m predictor \u001b[38;5;241m=\u001b[39m DeliveryPredictor()\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m dist_acc, time_acc \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelivery_data_pulito.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuratezza modello distanza: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdist_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuratezza modello tempo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 110\u001b[0m, in \u001b[0;36mDeliveryPredictor.train\u001b[1;34m(self, data_path)\u001b[0m\n\u001b[0;32m    105\u001b[0m X_train, X_test, y_dist_train, y_dist_test, y_time_train, y_time_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m    106\u001b[0m     X, y_distance, y_time, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dist_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_time_train)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:361\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labels=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m does not match number of samples=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(y), n_samples)\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 361\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43m_check_sample_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDOUBLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expanded_class_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2181\u001b[0m, in \u001b[0;36m_check_sample_weight\u001b[1;34m(sample_weight, X, dtype, copy, ensure_non_negative)\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2180\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 2181\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2182\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2183\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2186\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample weights must be 1D array or scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:116\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 116\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2466\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2463\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2467\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[0;32m   2469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#previsione non definitiva\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class DeliveryPredictor:\n",
    "    def __init__(self):\n",
    "        self.le_service = LabelEncoder()\n",
    "        self.le_vehicle = LabelEncoder()\n",
    "        self.le_dep_country = LabelEncoder()\n",
    "        self.le_arr_country = LabelEncoder()\n",
    "        self.le_dep_zip = LabelEncoder()\n",
    "        self.le_arr_zip = LabelEncoder()\n",
    "        self.scaler = RobustScaler()  # Cambiato a RobustScaler per gestire meglio gli outlier\n",
    "        self.distance_model = GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.time_model = GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def prepare_data(self, df):\n",
    "        # Pulisci e prepara i dati\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Converti i CAP in stringhe e gestisci i valori NaN\n",
    "        df['DEPARTURE_ZIPCODE'] = df['DEPARTURE_ZIPCODE'].fillna('00000')\n",
    "        df['ARRIVAL_ZIPCODE'] = df['ARRIVAL_ZIPCODE'].fillna('00000')\n",
    "        df['DEPARTURE_ZIPCODE'] = df['DEPARTURE_ZIPCODE'].astype(str).str.replace('.0', '').str.zfill(5)\n",
    "        df['ARRIVAL_ZIPCODE'] = df['ARRIVAL_ZIPCODE'].astype(str).str.replace('.0', '').str.zfill(5)\n",
    "        \n",
    "        # Rimuovi outlier dal tempo di consegna\n",
    "        q1 = df['DELIVERY_TIME_HH'].quantile(0.25)\n",
    "        q3 = df['DELIVERY_TIME_HH'].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df = df[\n",
    "            (df['DELIVERY_TIME_HH'] >= q1 - 1.5 * iqr) & \n",
    "            (df['DELIVERY_TIME_HH'] <= q3 + 1.5 * iqr)\n",
    "        ]\n",
    "        \n",
    "        # Aggiungi feature derivate\n",
    "        df['SAME_COUNTRY'] = (df['DEPARTURE_COUNTRY'] == df['ARRIVAL_COUNTRY']).astype(int)\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        df['SERVICETYPE_encoded'] = self.le_service.fit_transform(df['SERVICETYPE'])\n",
    "        df['VEHICLETYPE_encoded'] = self.le_vehicle.fit_transform(df['VEHICLETYPE'])\n",
    "        df['DEPARTURE_COUNTRY_encoded'] = self.le_dep_country.fit_transform(df['DEPARTURE_COUNTRY'])\n",
    "        df['ARRIVAL_COUNTRY_encoded'] = self.le_arr_country.fit_transform(df['ARRIVAL_COUNTRY'])\n",
    "        df['DEPARTURE_ZIPCODE_encoded'] = self.le_dep_zip.fit_transform(df['DEPARTURE_ZIPCODE'])\n",
    "        df['ARRIVAL_ZIPCODE_encoded'] = self.le_arr_zip.fit_transform(df['ARRIVAL_ZIPCODE'])\n",
    "        \n",
    "        # Store unique values\n",
    "        self.unique_values = {\n",
    "            'service_types': df['SERVICETYPE'].unique(),\n",
    "            'vehicle_types': df['VEHICLETYPE'].unique(),\n",
    "            'departure_countries': df['DEPARTURE_COUNTRY'].unique(),\n",
    "            'arrival_countries': df['ARRIVAL_COUNTRY'].unique(),\n",
    "            'departure_zipcodes': df['DEPARTURE_ZIPCODE'].unique(),\n",
    "            'arrival_zipcodes': df['ARRIVAL_ZIPCODE'].unique()\n",
    "        }\n",
    "        \n",
    "        # Prepare features\n",
    "        features = [\n",
    "            'SERVICETYPE_encoded', 'VEHICLETYPE_encoded', \n",
    "            'DEPARTURE_COUNTRY_encoded', 'ARRIVAL_COUNTRY_encoded',\n",
    "            'DEPARTURE_ZIPCODE_encoded', 'ARRIVAL_ZIPCODE_encoded', \n",
    "            'WDAY', 'SAME_COUNTRY'\n",
    "        ]\n",
    "        \n",
    "        # Aggiungi DISTANCE_ENCODED anche se non è presente nel dataset\n",
    "        if 'DECLARED_DISTANCE_KM' in df.columns:\n",
    "            df['DISTANCE_ENCODED'] = self.scaler.fit_transform(df[['DECLARED_DISTANCE_KM']])\n",
    "        else:\n",
    "            df['DISTANCE_ENCODED'] = 0  # Imposta a 0 se non è presente\n",
    "        \n",
    "        features.append('DISTANCE_ENCODED')\n",
    "        \n",
    "        X = df[features].values\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        return X, df\n",
    "\n",
    "    def train(self, data_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Prepare features and targets\n",
    "        X, df_cleaned = self.prepare_data(df)\n",
    "        y_distance = df_cleaned['DECLARED_DISTANCE_KM'].values\n",
    "        y_time = df_cleaned['DELIVERY_TIME_HH'].values\n",
    "        \n",
    "        # Normalizza il tempo di consegna per ore lavorative (8 ore al giorno)\n",
    "        y_time = np.minimum(y_time, 72)  # Cap a 72 ore (9 giorni lavorativi)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_dist_train, y_dist_test, y_time_train, y_time_test = train_test_split(\n",
    "            X, y_distance, y_time, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train models\n",
    "        self.distance_model.fit(X_train, y_dist_train)\n",
    "        self.time_model.fit(X_train, y_time_train)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        dist_accuracy = r2_score(y_dist_test, self.distance_model.predict(X_test))\n",
    "        time_accuracy = r2_score(y_time_test, self.time_model.predict(X_test))\n",
    "        \n",
    "        return dist_accuracy, time_accuracy\n",
    "    \n",
    "    def predict(self, service_type, vehicle_type, departure_country, arrival_country,\n",
    "                departure_zipcode, arrival_zipcode, departure_date):\n",
    "        # Convert departure_date to weekday (0 = Monday)\n",
    "        departure_dt = datetime.strptime(departure_date, '%Y-%m-%d')\n",
    "        wday = departure_dt.weekday()\n",
    "        \n",
    "        # Convert zipcodes to strings with leading zeros\n",
    "        departure_zipcode = str(departure_zipcode).replace('.0', '').zfill(5)\n",
    "        arrival_zipcode = str(arrival_zipcode).replace('.0', '').zfill(5)\n",
    "        \n",
    "        # Calculate same country feature\n",
    "        same_country = 1 if departure_country == arrival_country else 0\n",
    "        \n",
    "        # Prepare input data\n",
    "        input_data = np.array([[\n",
    "            self.le_service.transform([service_type])[0],\n",
    "            self.le_vehicle.transform([vehicle_type])[0],\n",
    "            self.le_dep_country.transform([departure_country])[0],\n",
    "            self.le_arr_country.transform([arrival_country])[0],\n",
    "            self.le_dep_zip.transform([departure_zipcode])[0],\n",
    "            self.le_arr_zip.transform([arrival_zipcode])[0],\n",
    "            wday,\n",
    "            same_country,\n",
    "            0  # Aggiungi un valore di default per DISTANCE_ENCODED\n",
    "        ]])\n",
    "        \n",
    "        input_scaled = self.scaler.transform(input_data)\n",
    "        \n",
    "        # Make predictions\n",
    "        predicted_distance = self.distance_model.predict(input_scaled)[0]\n",
    "        predicted_hours = self.time_model.predict(input_scaled)[0]\n",
    "        \n",
    "        # Limita le ore previste a un massimo ragionevole\n",
    "        predicted_hours = min(predicted_hours, 72)\n",
    "        \n",
    "        # Calcola il numero di giorni di lavoro e le ore di lavoro\n",
    "        work_hours_per_day = 8\n",
    "        total_workdays = int(np.ceil(predicted_hours / work_hours_per_day))\n",
    "        total_work_hours = total_workdays * work_hours_per_day\n",
    "        \n",
    "        # Calculate arrival date considering 8-hour workdays and weekends\n",
    "        arrival_date = departure_dt\n",
    "        \n",
    "        for _ in range(total_workdays):\n",
    "            arrival_date += timedelta(days=1)\n",
    "            # Skip weekends\n",
    "            while arrival_date.weekday() >= 5:  # 5 = Saturday, 6 = Sunday\n",
    "                arrival_date += timedelta(days=1)\n",
    "        \n",
    "        return {\n",
    "            'predicted_distance_km': round(predicted_distance, 2),\n",
    "            'predicted_hours': round(predicted_hours, 2),\n",
    "            'total_work_hours': total_work_hours,\n",
    "            'total_work_days': total_workdays,\n",
    "            'predicted_arrival_date': arrival_date.strftime('%Y-%m-%d'),\n",
    "            'weekend_adjusted': arrival_date.weekday() == 0 and departure_dt.weekday() < 5\n",
    "        }\n",
    "\n",
    "    def print_unique_values(self):\n",
    "        \"\"\"Print all unique values from the training data\"\"\"\n",
    "        print(\"\\nValori disponibili nel dataset:\")\n",
    "        print(\"\\nTipi di servizio:\", self.unique_values['service_types'])\n",
    "        print(\"\\nTipi di veicolo:\", self.unique_values['vehicle_types'])\n",
    "        print(\"\\nPaesi di partenza:\", self.unique_values['departure_countries'])\n",
    "        print(\"\\nPaesi di arrivo:\", self.unique_values['arrival_countries'])\n",
    "        print(\"\\nEsempio CAP di partenza:\", self.unique_values['departure_zipcodes'][:5])\n",
    "        print(\"\\nEsempio CAP di arrivo:\", self.unique_values['arrival_zipcodes'][:5])\n",
    "\n",
    "# Esempio di utilizzo\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = DeliveryPredictor()\n",
    "    \n",
    "    # Training\n",
    "    dist_acc, time_acc = predictor.train(\"delivery_data_pulito.csv\")\n",
    "    print(f\"Accuratezza modello distanza: {dist_acc*100:.2f}%\")\n",
    "    print(f\"Accuratezza modello tempo: {time_acc*100:.2f}%\")\n",
    "    \n",
    "    # Commenta o rimuovi la seguente riga se non vuoi stampare i valori disponibili\n",
    "    # predictor.print_unique_values()\n",
    "    \n",
    "    # Example prediction using actual values from the dataset\n",
    "    prediction = predictor.predict(\n",
    "        service_type=\"Corriere espresso\",\n",
    "        vehicle_type=\"Express\",\n",
    "        departure_country=\"IT\",\n",
    "        arrival_country=\"IT\",\n",
    "        departure_zipcode=\"62010\",\n",
    "        arrival_zipcode=\"22100\",\n",
    "        departure_date=\"2023-01-11\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPrevisione:\")\n",
    "    print(f\"Distanza stimata: {prediction['predicted_distance_km']} km\")\n",
    "    print(f\"Tempo stimato: {prediction['predicted_hours']} ore\")\n",
    "    print(f\"Ore di lavoro totali: {prediction['total_work_hours']} ore\")\n",
    "    print(f\"Giorni di lavoro totali: {prediction['total_work_days']} giorni\")\n",
    "    print(f\"Data di arrivo prevista: {prediction['predicted_arrival_date']}\")\n",
    "    if prediction['weekend_adjusted']:\n",
    "        print(\"Nota: La consegna è stata spostata a lunedì per il weekend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193c1449-7d50-4b43-b137-3871451b6705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza modello distanza: 100.00%\n",
      "Accuratezza modello tempo: 55.96%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8 features, but RobustScaler is expecting 9 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuratezza modello tempo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Example prediction using actual values from the dataset\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVia gomma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Cambia il tipo di servizio se necessario\u001b[39;49;00m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvehicle_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExpress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeparture_country\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43marrival_country\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeparture_zipcode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m62010\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43marrival_zipcode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m22100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeparture_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-01-11\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrevisione:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistanza stimata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_distance_km\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m km\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 143\u001b[0m, in \u001b[0;36mDeliveryPredictor.predict\u001b[1;34m(self, service_type, vehicle_type, departure_country, arrival_country, departure_zipcode, arrival_zipcode, departure_date)\u001b[0m\n\u001b[0;32m    133\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_service\u001b[38;5;241m.\u001b[39mtransform([service_type])[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    134\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_vehicle\u001b[38;5;241m.\u001b[39mtransform([vehicle_type])[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    135\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_dep_country\u001b[38;5;241m.\u001b[39mtransform([departure_country])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m                          wday,\n\u001b[0;32m    140\u001b[0m                          same_country]])\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Scale input data\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m input_data_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Predict distance and time\u001b[39;00m\n\u001b[0;32m    146\u001b[0m predicted_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_model\u001b[38;5;241m.\u001b[39mpredict(input_data_scaled)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1686\u001b[0m, in \u001b[0;36mRobustScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Center and scale the data.\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;124;03m    Transformed array.\u001b[39;00m\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1686\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_scaling:\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\Desktop\\Data_Science\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8 features, but RobustScaler is expecting 9 features as input."
     ]
    }
   ],
   "source": [
    "#prova Previsione\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class DeliveryPredictor:\n",
    "    def __init__(self):\n",
    "        self.le_service = LabelEncoder()\n",
    "        self.le_vehicle = LabelEncoder()\n",
    "        self.le_dep_country = LabelEncoder()\n",
    "        self.le_arr_country = LabelEncoder()\n",
    "        self.le_dep_zip = LabelEncoder()\n",
    "        self.le_arr_zip = LabelEncoder()\n",
    "        self.scaler = RobustScaler()  # Cambiato a RobustScaler per gestire meglio gli outlier\n",
    "        self.distance_model = GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.time_model = GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def prepare_data(self, df):\n",
    "        # Pulisci e prepara i dati\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Converti i CAP in stringhe e gestisci i valori NaN\n",
    "        df['DEPARTURE_ZIPCODE'] = df['DEPARTURE_ZIPCODE'].fillna('00000')\n",
    "        df['ARRIVAL_ZIPCODE'] = df['ARRIVAL_ZIPCODE'].fillna('00000')\n",
    "        df['DEPARTURE_ZIPCODE'] = df['DEPARTURE_ZIPCODE'].astype(str).str.replace('.0', '').str.zfill(5)\n",
    "        df['ARRIVAL_ZIPCODE'] = df['ARRIVAL_ZIPCODE'].astype(str).str.replace('.0', '').str.zfill(5)\n",
    "        \n",
    "        # Rimuovi outlier dal tempo di consegna\n",
    "        q1 = df['DELIVERY_TIME_HH'].quantile(0.25)\n",
    "        q3 = df['DELIVERY_TIME_HH'].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df = df[\n",
    "            (df['DELIVERY_TIME_HH'] >= q1 - 1.5 * iqr) & \n",
    "            (df['DELIVERY_TIME_HH'] <= q3 + 1.5 * iqr)\n",
    "        ]\n",
    "        \n",
    "        # Aggiungi feature derivate\n",
    "        df['SAME_COUNTRY'] = (df['DEPARTURE_COUNTRY'] == df['ARRIVAL_COUNTRY']).astype(int)\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        df['SERVICETYPE_encoded'] = self.le_service.fit_transform(df['SERVICETYPE'])\n",
    "        df['VEHICLETYPE_encoded'] = self.le_vehicle.fit_transform(df['VEHICLETYPE'])\n",
    "        df['DEPARTURE_COUNTRY_encoded'] = self.le_dep_country.fit_transform(df['DEPARTURE_COUNTRY'])\n",
    "        df['ARRIVAL_COUNTRY_encoded'] = self.le_arr_country.fit_transform(df['ARRIVAL_COUNTRY'])\n",
    "        df['DEPARTURE_ZIPCODE_encoded'] = self.le_dep_zip.fit_transform(df['DEPARTURE_ZIPCODE'])\n",
    "        df['ARRIVAL_ZIPCODE_encoded'] = self.le_arr_zip.fit_transform(df['ARRIVAL_ZIPCODE'])\n",
    "        \n",
    "        # Store unique values\n",
    "        self.unique_values = {\n",
    "            'service_types': df['SERVICETYPE'].unique(),\n",
    "            'vehicle_types': df['VEHICLETYPE'].unique(),\n",
    "            'departure_countries': df['DEPARTURE_COUNTRY'].unique(),\n",
    "            'arrival_countries': df['ARRIVAL_COUNTRY'].unique(),\n",
    "            'departure_zipcodes': df['DEPARTURE_ZIPCODE'].unique(),\n",
    "            'arrival_zipcodes': df['ARRIVAL_ZIPCODE'].unique()\n",
    "        }\n",
    "        \n",
    "        # Prepare features\n",
    "        features = [\n",
    "            'SERVICETYPE_encoded', 'VEHICLETYPE_encoded', \n",
    "            'DEPARTURE_COUNTRY_encoded', 'ARRIVAL_COUNTRY_encoded',\n",
    "            'DEPARTURE_ZIPCODE_encoded', 'ARRIVAL_ZIPCODE_encoded', \n",
    "            'WDAY', 'SAME_COUNTRY'\n",
    "        ]\n",
    "        \n",
    "        # Aggiungi DISTANCE_ENCODED anche se non è presente nel dataset\n",
    "        if 'DECLARED_DISTANCE_KM' in df.columns:\n",
    "            df['DISTANCE_ENCODED'] = self.scaler.fit_transform(df[['DECLARED_DISTANCE_KM']])\n",
    "        else:\n",
    "            df['DISTANCE_ENCODED'] = 0  # Imposta a 0 se non è presente\n",
    "        \n",
    "        features.append('DISTANCE_ENCODED')\n",
    "        \n",
    "        X = df[features].values\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        return X, df\n",
    "\n",
    "    def train(self, data_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Prepare features and targets\n",
    "        X, df_cleaned = self.prepare_data(df)\n",
    "        y_distance = df_cleaned['DECLARED_DISTANCE_KM'].values\n",
    "        y_time = df_cleaned['DELIVERY_TIME_HH'].values\n",
    "        \n",
    "        # Normalizza il tempo di consegna per ore lavorative (8 ore al giorno)\n",
    "        y_time = np.minimum(y_time, 72)  # Cap a 72 ore (9 giorni lavorativi)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_dist_train, y_dist_test, y_time_train, y_time_test = train_test_split(\n",
    "            X, y_distance, y_time, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train models\n",
    "        self.distance_model.fit(X_train, y_dist_train)\n",
    "        self.time_model.fit(X_train, y_time_train)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        dist_accuracy = r2_score(y_dist_test, self.distance_model.predict(X_test))\n",
    "        time_accuracy = r2_score(y_time_test, self.time_model.predict(X_test))\n",
    "        \n",
    "        return dist_accuracy, time_accuracy\n",
    "    \n",
    "    def predict(self, service_type, vehicle_type, departure_country, arrival_country,\n",
    "                departure_zipcode, arrival_zipcode, departure_date):\n",
    "        # Convert departure_date to weekday (0 = Monday)\n",
    "        departure_dt = datetime.strptime(departure_date, '%Y-%m-%d')\n",
    "        wday = departure_dt.weekday()\n",
    "        \n",
    "        # Convert zipcodes to strings with leading zeros\n",
    "        departure_zipcode = str(departure_zipcode).replace('.0', '').zfill(5)\n",
    "        arrival_zipcode = str(arrival_zipcode).replace('.0', '').zfill(5)\n",
    "        \n",
    "        # Calculate same country feature\n",
    "        same_country = 1 if departure_country == arrival_country else 0\n",
    "        \n",
    "        # Prepare input data\n",
    "        input_data = np.array([[self.le_service.transform([service_type])[0],\n",
    "                                 self.le_vehicle.transform([vehicle_type])[0],\n",
    "                                 self.le_dep_country.transform([departure_country])[0],\n",
    "                                 self.le_arr_country.transform([arrival_country])[0],\n",
    "                                 self.le_dep_zip.transform([departure_zipcode])[0],\n",
    "                                 self.le_arr_zip.transform([arrival_zipcode])[0],\n",
    "                                 wday,\n",
    "                                 same_country]])\n",
    "        \n",
    "        # Scale input data\n",
    "        input_data_scaled = self.scaler.transform(input_data)\n",
    "        \n",
    "        # Predict distance and time\n",
    "        predicted_distance = self.distance_model.predict(input_data_scaled)[0]\n",
    "        predicted_time = self.time_model.predict(input_data_scaled)[0]\n",
    "        \n",
    "        # Calcola la data di arrivo\n",
    "        arrival_date = departure_dt + timedelta(hours=predicted_time)\n",
    "        \n",
    "        # Adjust arrival date if service type is not E-commerce\n",
    "        if service_type != \"E-commerce\":\n",
    "            if arrival_date.weekday() == 5:  # Saturday\n",
    "                arrival_date += timedelta(days=2)  # Move to Monday\n",
    "            elif arrival_date.weekday() == 6:  # Sunday\n",
    "                arrival_date += timedelta(days=1)  # Move to Monday\n",
    "        \n",
    "        return {\n",
    "            'predicted_distance_km': round(predicted_distance, 2),\n",
    "            'predicted_hours': round(predicted_time, 2),\n",
    "            'predicted_arrival_date': arrival_date.strftime('%Y-%m-%d')\n",
    "        }\n",
    "\n",
    "# Esempio di utilizzo\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = DeliveryPredictor()\n",
    "    \n",
    "    # Training\n",
    "    dist_acc, time_acc = predictor.train(\"delivery_data_pulito.csv\")\n",
    "    print(f\"Accuratezza modello distanza: {dist_acc*100:.2f}%\")\n",
    "    print(f\"Accuratezza modello tempo: {time_acc*100:.2f}%\")\n",
    "    \n",
    "    # Example prediction using actual values from the dataset\n",
    "    prediction = predictor.predict(\n",
    "        service_type=\"Via gomma\",  # Cambia il tipo di servizio se necessario\n",
    "        vehicle_type=\"Express\",\n",
    "        departure_country=\"IT\",\n",
    "        arrival_country=\"IT\",\n",
    "        departure_zipcode=\"62010\",\n",
    "        arrival_zipcode=\"22100\",\n",
    "        departure_date=\"2023-01-11\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPrevisione:\")\n",
    "    print(f\"Distanza stimata: {prediction['predicted_distance_km']} km\")\n",
    "    print(f\"Tempo stimato: {prediction['predicted_hours']} ore\")\n",
    "    print(f\"Data di arrivo prevista: {prediction['predicted_arrival_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a4808-1485-4260-9fb8-1bca03d177c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
